{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef0d7ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda03\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ASUS\\anaconda03\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ASUS\\anaconda03\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there! I'm Buddy.\n",
      "I'm here to provide comfort and support.\n",
      "You: i am not feeling good suggest me something\n",
      "Bot: You: i am not feeling good suggest me something\n",
      "Bot: no you are not.\n",
      "bot: what do you mean by that?\n",
      "chatty: I am feeling bad about this. i have no idea what to do about it. it is not my fault, i just don't know how to deal with it and i'm not sure what i can do to make it go away. but i do know that this is something i need to talk to you about and that is why i want you to come to me and tell me what you want to hear. if you have any questions or concerns please feel free to contact me. I will try my best to answer them as soon as possible.\n",
      "You: byd\n",
      "Bot: You: byd\n",
      "Bot: I don't know what you're talking about, but I'm going to try to help you out. If you want to talk to me, I'll be happy to answer any questions you may have.\n",
      "I'm sorry, you can't answer my questions. I just want you to know that I love you, and I will do my best to make sure that you have the best experience possible with me. You can always contact me via email if you'd like to ask me a question, or I can answer your questions in person. Thank you very much for your time and understanding.\n",
      "\n",
      "\n",
      "Thank you for taking the time out of your busy schedule to read this. It's been a pleasure.\n",
      "You: bye\n",
      "Remember, I'm just a message away.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "class EmotionalSupportAnimalChatbot:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        self.model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "        self.responses = {\n",
    "            'greeting': [\"Hello! I'm {} your emotional support companion.\", \"Hi there! I'm {}.\", \"Hey! I'm {}.\"],\n",
    "            'comfort': [\"I'm here to provide comfort and support.\", \"You're not alone. I'm here for you.\", \"Feel free to share your thoughts with me.\"],\n",
    "            'encouragement': [\"You're doing great!\", \"Remember, each day is a new opportunity.\", \"I believe in you!\"],\n",
    "            'farewell': [\"Take care! Reach out whenever you need.\", \"Goodbye for now! I'll be here whenever you want to chat.\", \"Remember, I'm just a message away.\"]\n",
    "        }\n",
    "\n",
    "    def generate_response(self, input_text):\n",
    "        model_input = f\"You: {input_text}\\nBot:\"\n",
    "        input_ids = self.tokenizer.encode(model_input, return_tensors=\"tf\")\n",
    "        attention_mask = tf.ones(input_ids.shape, dtype=input_ids.dtype)\n",
    "        output_ids = self.model.generate(input_ids, attention_mask=attention_mask, max_length=150, num_beams=5, no_repeat_ngram_size=2, top_k=50, pad_token_id=self.tokenizer.eos_token_id, eos_token_id=self.tokenizer.eos_token_id, do_sample=True, temperature=0.7)\n",
    "        response = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        return response\n",
    "\n",
    "    def get_response(self, category):\n",
    "        return random.choice(self.responses[category])\n",
    "\n",
    "    def start_conversation(self):\n",
    "        print(self.get_response('greeting').format(self.name))\n",
    "        print(self.get_response('comfort'))\n",
    "\n",
    "    def chat_loop(self):\n",
    "        self.start_conversation()\n",
    "\n",
    "        while True:\n",
    "            user_input = input(\"You: \").lower()\n",
    "\n",
    "            if 'bye' in user_input:\n",
    "                print(self.get_response('farewell'))\n",
    "                break\n",
    "            elif 'encourage' in user_input:\n",
    "                print(self.get_response('encouragement'))\n",
    "            else:\n",
    "                model_response = self.generate_response(user_input)\n",
    "                print(f\"Bot: {model_response}\")\n",
    "\n",
    "# Create an instance of the EmotionalSupportAnimalChatbot\n",
    "bot_name = \"Buddy\"\n",
    "esa_bot = EmotionalSupportAnimalChatbot(bot_name)\n",
    "\n",
    "# Start the chat loop\n",
    "esa_bot.chat_loop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4f95dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there! I'm Buddy.\n",
      "I'm here to provide comfort and support.\n",
      "You: i am happy today\n",
      "\n",
      "Bot: i am happy today to announce that we have signed a new agreement with the United States Department of Justice (DOJ) and the Federal Bureau of Investigation (FBI). This agreement will allow us to continue our efforts to protect the privacy and security of our customers, and we look forward to working with you in the future.\"\n",
      "\n",
      "The agreement is part of a larger effort by the DOJ and FBI to improve the security and privacy of U.S. businesses. The agreement also allows the FBI and DOJ to work together to develop and implement new policies and procedures to help protect customers' personal information. In addition, the agreement allows for the use of existing law enforcement and intelligence agencies to share information with each other to ensure that the information is protected.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "class EmotionalSupportAnimalChatbot:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        self.model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "        self.responses = {\n",
    "            'greeting': [\"Hello! I'm {} your emotional support companion.\", \"Hi there! I'm {}.\", \"Hey! I'm {}.\"],\n",
    "            'comfort': [\"I'm here to provide comfort and support.\", \"You're not alone. I'm here for you.\", \"Feel free to share your thoughts with me.\"],\n",
    "            'encouragement': [\"You're doing great!\", \"Remember, each day is a new opportunity.\", \"I believe in you!\"],\n",
    "            'farewell': [\"Take care! Reach out whenever you need.\", \"Goodbye for now! I'll be here whenever you want to chat.\", \"Remember, I'm just a message away.\"]\n",
    "        }\n",
    "\n",
    "    def generate_response(self, input_text):\n",
    "        # Use a prompt to guide the model\n",
    "        prompt = str(input_text)\n",
    "    \n",
    "        # Encode the prompt\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"tf\")\n",
    "    \n",
    "        # Generate the response\n",
    "        output_ids = self.model.generate(\n",
    "            input_ids,\n",
    "            max_length=150,\n",
    "            num_beams=5,\n",
    "            no_repeat_ngram_size=2,\n",
    "            top_k=50,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.7\n",
    "            )\n",
    "    \n",
    "        # Decode the response and skip special tokens\n",
    "        response = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "        return response\n",
    "\n",
    "\n",
    "    def get_response(self, category):\n",
    "        return random.choice(self.responses[category])\n",
    "\n",
    "    def start_conversation(self):\n",
    "        print(self.get_response('greeting').format(self.name))\n",
    "        print(self.get_response('comfort'))\n",
    "\n",
    "    def chat_loop(self):\n",
    "        self.start_conversation()\n",
    "\n",
    "        while True:\n",
    "            user_input = input(\"You: \").lower()\n",
    "\n",
    "            if 'bye' in user_input:\n",
    "                print(self.get_response('farewell'))\n",
    "                break\n",
    "            elif 'encourage' in user_input:\n",
    "                print(self.get_response('encouragement'))\n",
    "            else:\n",
    "                model_response = self.generate_response(user_input)\n",
    "                print(f\"\\nBot: {model_response}\")\n",
    "\n",
    "# Create an instance of the EmotionalSupportAnimalChatbot\n",
    "bot_name = \"Buddy\"\n",
    "esa_bot = EmotionalSupportAnimalChatbot(bot_name)\n",
    "\n",
    "# Start the chat loop\n",
    "esa_bot.chat_loop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dccef1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
